\section{Discussion}\label{section:discussion}

As stated earlier in the section ``Aim of the thesis'' (\ref{section:aim}), we set out to answer two research questions:

\paragraph*{RQ1. Can we improve upon previous results in classifying code vs prose comprehension with EEG~?}

We found that we do improve upon the best classifier performance by Fucci et al (in their EEG-only configuration). However, we had to remove certain subjects from our sample, and our sample in general was smaller and more homogenous. 

Our results are impacted by our cherry-picked subject set for our final analysis. Issues with signal quality and limited data (due to spurious device disconnects during experiments) ultimately led to exclusion of several subjects. However, despite these interventions we find that Riemannian-based classifiers outperforms using bandpower-features (as by Fucci et al.), this suggests some improvement of our method over previous work.

We were not able to outperform the results by Floyd et al., which is not surprising given they were using using fMRI\@.

\paragraph*{RQ2. Can we train an EEG classifier to separate software developersâ€™ device activities?}

We successfully train several classifiers. We especially distinguishing work (writing code) from social media (Twitter), but many models show significant improvements when given more data. We \todo{explore}{not yet} explore a multiclass classifier, but\ldots

Among the classifiers we train, we found that discerning leisure-activities from each other (such as YouTube vs Twitter) is harder than discerning work from leisure (such as writing code vs Twitter).

\add[inline]{More discussion of results}

\subsection{Applications to software engineering}

As we mentioned in the introduction, gaining insight into the brains of developers at work can be used to aid and enhance the productivity of developers in several ways.

Some to assist in development, like helping to identify developer confusion, but other applications could be imagined where a summary of the developers' brain activity during a particular commit (such as if the developer was confused, focused, tired).

\add[inline]{Include mention and image of mood attached in commit message from Fucci talk?}

\subsection{Applications to the average device user}

\change[inline]{Merge with applications to software engineering?}

Applications of our results include:

\begin{itemize}
    \item Use the confidence in the task prediction as an alternative measurement of focus (not merely measuring that the subject was focused, but what they were focused on). 
    \item Detecting distraction.
\end{itemize}

\subsection{Threats to validity}\label{section:threats}

    During our work we have considered several potential threats to validity. Some of these arise from our limited and biased dataset, while others are about the task and methodology.

    Starting with our dataset, we collected on mostly right-handed males in their late 20s. This uniform/homogenous sample may lead to less data needed to train a classifier for that particular group, but does not necessarily generalize well to the population at large. Future studies should include a more diverse sample of participants.

    We also had issues during data collection with spurious disconnects from the device, leading to data loss and incomplete experiment runs. This is a threat to the validity of the study due to not all subjects having undergone as many (or the same) trials.

    Among our considerations, one threat to the validity is the stimuli images themselves (seen in Figure~\ref{fig:tasks}). In Floyd et al.~the images used for prose comprehension are in fact more like a code review task, while Fucci et al.~modified the images to test comprehension, instead of ability to judge correctness. We also discovered that subjects found the prose stimuli used by Floyd et al.~confusing, and it would have been preferrable to use prose stimuli more like that used by Fucci et al.

    The stimuli images differ on more than just content. Examples of such differences are the background color, the difference in eye saccades while reading (eyes jump around more during the code tasks, where the user may have to jump between the code and the question about it). We have not been able to discard the possibility that the front-heavy electrode placements (with reference electrode at Fpz) lead to much of the signal being from eye saccades. 

    Future research could evaluate this further either by using an eye tracker or by using an EEG device with different placements of both the electrodes and reference electrode.

    \add[inline]{Investigate threats to validity mentioned by previous authors}

\subsection{Ethical considerations}

    When studying EEG data a range of ethical considerations arise. 

    \begin{itemize}
        \item Could the data be considered personally identifiable information (PII)? 
        \item How privacy sensitive are EEG recordings? Could they contain something the subject would rather keep private? (could have medical implications)
        \item How do we build large public datasets, while preserving participants privacy?
    \end{itemize}

    Companies such as Neurosity have taken an approach with their products where all the processing happens on-device, and only aggregates and classifier outputs are sent to the cloud for storage and presentation to the user.

    \add[inline]{Discuss ethics/privacy considerations of data collection, how it's dealt with in ActivityWatch, and implications of results on similar concerns apply to EEG data}

    \add[inline]{Mention OpenMined, https://github.com/OpenMined/PySyft, and similar tech (esp in the context of crowdsourcing data)}

\subsection{Democratization of neuroscience}

    This thesis was made possible due to the efforts of individuals and communities such as NeuroTechX to democratize neuroscience. Indeed, it is the explicit goal of the NeuroTechX eeg-notebooks project to `democratize the neuroscience experiment'. Combined with the rapid cost reduction of research-grade EEG equipment over the last decade it has enabled any programmer to design and perform high-quality neuroscience experiments.

    As development of BCIs advance and the consumer market for EEG devices grow (as evidenced by new devices being released with a regular cadence by InteraXon and Neurosity) we expect to see more uses and applications of these devices.

    Much of this work was made possible due to the efforts of communities such as NeuroTechX to democratize neuroscience by publishing tools and code for running experiments.

\subsection{Crowdsourcing data}

    Collecting data is a significant time sink for researchers, and efforts to crowdsource data from the general public are difficult for EEG as it still requires access to the equipment, the knowledge to operate it, as well as considerations like signal quality, electrode placement, and other factors that might invalidate the data.

    As part of the thesis work I have contributed to an effort in crowdsourcing EEG data collected from the experiments built in eeg-notebooks, using consumer EEG devices like those from Muse, OpenBCI, and Neurosity\@. The effort, called the \href{https://neurotech-challenge.com/}{NeuroTech Challenge Series} (NTCS), is lead by John Griffiths at the University of Toronto.

    Crowdsourcing data comes with new challenges. One of them is that data is now recorded by many different devices, with differences in channel count, sampling rate, electrode placement, and so on, that are difficult to combine in the same dataset. 

    A potential solution to the problem of learning new variations from a small additional sample problem is to attempt cross-task or cross-subject transfer learning. In such a setup, a model is already trained on a large amount of subjects, or many tasks, and through additional collected data can self-improve and adapt to one user or one task.

    \add[inline]{Write about crowdsourcing of EEG data, including the potential of transfer learning and privacy considerations.}


\subsection{Transfer learning}

An important aspect, as highlighted earlier in this thesis, is the ability of a classifier to be able to work on subjects unseen in training. Some BCI systems employ a calibration phase to achieve greater performance, but it is time consuming and can be straining. 

To minimize, and perhaps even eliminate, the need for this calibration is a stated research goal in Khazem et al.~\cite{khazem_minimizing_2021}, which presents what is called Riemannian Transfer Learning. They build on Riemannian geometry used in state-of-the-art classifiers, and find that\add{TODO}{\ldots}
