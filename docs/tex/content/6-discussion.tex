\chapter{Discussion}\label{section:discussion}

We now move on to discussing our results and answering our two research questions. We will also discuss some threats to the validity, potential applications, ethical considerations, and a brief mention of research initiatives we have contributed to along the way.

\section{RQ1 --- Improving upon previous results in classifying code vs prose}

We found that we do improve upon the best classifier performance by Fucci et al.\ in their EEG-only configuration~\cite{fucci_replication_2019}. However, we had to remove certain subjects from our sample, and our sample in general was smaller and more homogenous.\footnote{The reason for this cherry-picked subject selection is due to issues with signal quality and limited data (due to spurious device disconnects).}

Our findings also show that Riemannian methods, as used in this thesis, are better at distinguishing between the code and prose tasks than the bandpower-features approach, as used by Fucci et al. Our Riemannian score\footnote{Scores taken from Table~\ref{table:compare-results}} was $0.75$, which is better than both our bandpower-features score of $0.69$, as well as the Fucci et al score of $0.66$. This is in line with our expectations, as Riemannian methods are considered state-of-the-art for many EEG and BCI tasks, unlike bandpower features.

We were not able to outperform the results by Floyd et al.~\cite{floyd_decoding_2017}. This is not surprising given they were using fMRI, which has far superior spatial resolution and can therefore accurately detect elevated activity in specific brain regions over the duration of an epoch. However, our method using EEG has the benefit of high temporal resolution, allowing classification using small amounts of data, enabling near real-time applications.

\section{RQ2 --- Identify software developers' work tasks based on brain activity}

We train several classifiers and achieve promising results. We were especially successful in distinguishing work (writing code) from social media (Twitter), with a BAC of $0.69$.

Among the classifiers we train, we found that discerning leisure-activities from each other (such as YouTube vs Twitter, with a BAC of $0.60$) is harder than discerning work from leisure (such as writing code vs Twitter). We also find that EEG is sufficient to not only pick up differences in code vs prose \emph{comprehension} but also in \emph{writing}, as shown by our writing code vs writing prose categories (BAC of $0.68$).

We conclude that our preliminary results indicate that it is possible to discern several different device activities using EEG\@. However, more data from multiple subjects are needed in future work to validate the method and results. We also want to note that we only try to discern two types of activity at a time, pair by pair, and do not try to identify them among all possible work activities (a much more difficult task, left for future work).

\section{Threats to validity}\label{section:threats}

    During our work we have considered several potential threats to validity. Some of these arise from our limited and biased dataset, while others are about the task and methodology.

    Starting with our dataset, we collected on mostly right-handed males in their late 20s. This uniform/homogenous sample may lead to less data needed to train a classifier for that particular group, but does not necessarily generalize well to the population at large. Future studies should include a more diverse sample of participants.

    We also had issues during data collection with spurious disconnects from the device, leading to data loss and incomplete experiment runs. This is a threat to the validity of the study due to not all subjects having undergone as many (or the same) trials.

    Among our considerations, one threat to the validity is the stimuli images themselves (seen in Figure~\ref{fig:tasks}). In Floyd et al.~the images used for prose comprehension are in fact more like a code review task, while Fucci et al.~modified the images to test comprehension, instead of ability to judge correctness. We also discovered that subjects found the prose stimuli used by Floyd et al.~confusing, and it would have been preferable to use prose stimuli more like that used by Fucci et al.

    The stimuli images differ on more than just content. Examples of such differences are the background color, the difference in eye saccades while reading (eyes jump around more during the code tasks, where the user may have to jump between the code and the question about it). We have not been able to discard the possibility that the front-heavy electrode placements (with reference electrode at Fpz) lead to much of the signal being from eye saccades. 

    Future research could evaluate this further either by using an eye tracker or by using an EEG device with different placements of both the electrodes and reference electrode.

    %\add[inline]{Investigate threats to validity mentioned by previous authors}

\section{Applications to software engineering}

Gaining insight into the minds of developers at work can be used to aid and enhance the productivity of developers in several ways. As an example, it could be used to assist the developer in real time, like helping to identify developer confusion, but other applications could be imagined where a summary of the developers' brain activity during a particular commit (such as if the developer was confused, focused, tired) is included in the commit message, to help identify commits prone to introducing bugs.

%\add[inline]{Include mention and image of mood attached in commit message from Fucci talk?}

Applications of our results include:

\begin{itemize}
    \item Use the confidence in the task prediction as an alternative measurement of focus (not merely measuring that the subject was focused, but what they were focused on). 
    \item Detecting distraction.
    %\item The ability to collect all data needed to train an EEG classifier during normal device use.
\end{itemize}

We also consider our novel idea of `mental churn', a neurally integrated alternative to the measure of code churn, where not only code changes are measured, but also the \emph{attention} a file or piece of code receives as a whole. It can be viewed as quantifying the \emph{cognitive load} of the developer, and attributing it to the current context. This notion of mental churn could also include what response the code elicited in an engineer (confusion, focus, confidence).

\section{Ethical considerations}

    When studying EEG data a range of ethical considerations arise. 

    \begin{itemize}
        \item Could the data be considered personally identifiable information (PII) and thereby fall under GDPR's regulations concerning ``sensitive user data''? 
        \item How privacy sensitive are EEG recordings? Could they contain something the subject would rather keep private? (could have medical implications)
        \item How do we build large public datasets, while preserving participants privacy?
        %\item How can one use EEG in a work environment without unreasonable surveillance of the employee?
    \end{itemize}

    Companies such as Neurosity have taken an approach with their products where all the processing happens on-device, and only aggregates and classifier outputs are sent to the cloud for storage and presentation to the user. This is similar to the approach taken by ActivityWatch for device usage data, where all data is stored locally and never sent to a remote server for processing. 

    We believe this approach is the most privacy-preserving, but it comes at the cost of difficulty in building large datasets, as the data is no longer collected in a central repository under the control of companies or service providers. Therefore, there is a need to bridge the gap between privacy and sharing data.

    Simple solutions to this problem include opt-in to data collection, which is easy to do at scale for companies offering EEG devices. But other efforts include projects such as the NeuroTech Challenge (mentioned in Section~\ref{section:ntcs}).

    There are also more advanced solutions, such as privacy-preserving ML systems. One example of such solutions is \href{https://github.com/OpenMined/PySyft}{PySyft}, which enables private deep learning using federated learning, differential privacy, and encrypted computation (through Multi-Party Computation and Homomorphic Encryption) to work within common deep learning frameworks such as PyTorch and Tensorflow.

\section{Democratization of neuroscience}

    This thesis was made possible due to the efforts of individuals and communities such as NeuroTechX to democratize neuroscience. Indeed, it is the explicit goal of the NeuroTechX eeg-notebooks project to `democratize the neuroscience experiment'. Combined with the rapid cost reduction of research-grade EEG equipment over the last decade it has enabled hobbyists to design and perform high-quality neuroscience experiments. This enables citizen scientists to contribute to data collection, an example of such an effort is the NeuroTech Challenge Series (described in Section~\ref{section:ntcs}).

    As development of BCIs advance and the consumer market for EEG devices grow (as evidenced by new devices being released with a regular cadence by InteraXon and Neurosity) we expect to see more uses and applications of these devices.

    Much of this work was made possible due to the efforts of communities such as NeuroTechX to democratize neuroscience by publishing tools for running experiments. As part of the thesis, we have contributed changes back to some of the tools used (as mentioned in~\Vref{section:aim}).

\subsection{Crowdsourcing data}

    Collecting data is a significant time sink for researchers, and efforts to crowdsource data from the general public are difficult for EEG as it still requires access to the equipment, the knowledge to operate it, as well as considerations like signal quality, electrode placement, and other factors that might invalidate the data.

    Crowdsourcing data comes with new challenges. One of them is that data is now recorded by many different devices, with differences in channel count, sampling rate, electrode placement, and so on, that are difficult to combine in the same dataset. 

    A potential solution to the problem of learning new variations from a small additional sample is to attempt cross-task or cross-subject transfer learning. In such a setup, a model is already trained on a group of subjects, or a similar task, and through a small amount of new data for a particular subject or task, the model can adapt to learn those previously unseen subjects or tasks. This is discussed further in Section~\ref{section:transfer-learning}.

    %\paragraph*{NeuroTech Challenge Series}

    As part of the thesis work I have contributed to the \href{https://neurotech-challenge.com/}{NeuroTech Challenge Series} (NTCS), an effort in crowdsourcing EEG data using the experiments built in \texttt{eeg-notebooks}. The challenge can be performed by anyone with access mobile consumer EEG devices, like those from Muse, OpenBCI, and Neurosity\@. The project is a collaborative effort led by John Griffiths at the University of Toronto, with support from OpenBCI and NeuroTechX.\label{section:ntcs}


\section{Transfer learning}\label{section:transfer-learning}

An important aspect, as highlighted earlier in this thesis, is the ability of a classifier to be able to work on subjects unseen in training. Some BCI systems employ a calibration phase to achieve greater performance, but this can be time-consuming and straining for the user.

To minimize this need for calibration is a stated research goal in Khazem et al.~\cite{khazem_minimizing_2021}, which presents what is called Riemannian Transfer Learning. They build on Riemannian geometry used in state-of-the-art classifiers, and develop a variation of MDM (explained in Section~\ref{section:riemannian-theory}) called \emph{Minimum Distance to Weighted Mean} (MDWM). The method takes a parameter $\lambda$ (where $0 \leq \lambda \leq 1$) that controls how much the algorithm should rely on the class centroids learned from past subjects versus the calibration data from a new subject. This is useful in online learning contexts, where the parameter can initially start at $0$ and then be incrementally adjusted towards $1$. The researchers found $\lambda = 0.7$ to be a reasonable value for many tasks. 

The researchers also considered using weights for each source subject as done by Kalunga et al.~\cite{kalunga_transfer_2018}, which could be used to adjust for subject similarity.

We investigated this avenue of inquiry to potentially minimize the amount of data collection needed, but in the end did not have the time to implement it.
