{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main analysis\n",
    "\n",
    "The primary analysis for the thesis, where we train a classifier for the code vs prose task.\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "    \n",
    " - [Setup](#Setup)\n",
    "   - Imports\n",
    "   - Configuration\n",
    " - [Loading](#Loading)\n",
    "   - Loading EEG data\n",
    "   - Loading markers\n",
    " - [Preprocessing](#Preprocessing)\n",
    "   - Filter short trials\n",
    "   - Filter no answer trials\n",
    "   - Bandpass filtering\n",
    "   - Constructing epochs\n",
    "   - Epochs to windows\n",
    "   - Constructing our `X` and `y`\n",
    " - [Training](#Training)\n",
    "   - Learning curves\n",
    "      \n",
    "**NOTE:** This TOC is manually built and may not be up to date.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "\n",
    "from eegclassify import main, load, clean, features, preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell contains all the configuration options available for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "use_bandpass_filter = True\n",
    "classify_breaks = False\n",
    "single_subject = None    # set to subject number, or False\n",
    "balance_dataset = True\n",
    "min_task_duration = 5\n",
    "standardize = False\n",
    "    \n",
    "# Constants\n",
    "sfreq = 256  # sampling frequency of the Muse S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(format=\"%(levelname)s: %(msg)s\", level=logging.INFO, force=True)\n",
    "logger = logging.getLogger(__name__)\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "# Set this to True to run on testing data\n",
    "simulate_test = False\n",
    "if simulate_test:\n",
    "    import os\n",
    "    os.environ['PYTEST_CURRENT_TEST'] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "document.title='erb-thesis/Main - Jupyter'  // Set the document title to be able to track time spent working on the notebook with ActivityWatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading EEG\n",
    "\n",
    "First we need to load the EEG data used during the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data').resolve()\n",
    "    \n",
    "files = sorted([\n",
    "    data_dir / \"eeg/muse/subject0000/session001/recording_2021-04-02-14.03.36.csv\",\n",
    "    *list((data_dir / \"eeg/muse/subject0001/session001/\").glob(\"*.csv\")),\n",
    "])\n",
    "pprint(files)\n",
    "\n",
    "eeg = load.load_eeg(files)\n",
    "eeg = eeg.set_index('timestamp').sort_index()\n",
    "print(eeg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the loaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eeg(X, offset, window, span=None):\n",
    "    plt.figure(figsize=(20, 3))\n",
    "    data = X[offset : offset + window, :]\n",
    "    plt.plot(data)\n",
    "    plt.xlim(0, window);\n",
    "    if span is None:\n",
    "        span = max(np.std(data, axis=1))\n",
    "    plt.ylim(-span, span);\n",
    "\n",
    "plot_eeg(eeg.to_numpy(), offset = 1000, window = 3 * sfreq)\n",
    "plot_eeg(eeg.to_numpy(), offset = 20000, window = 3 * sfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some channels are bad some of the time, we will deal with that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading markers\n",
    "\n",
    "Now we need to load the markers produced during each trial of the experiment, so we can annotate the EEG data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_files = [\n",
    "    data_dir / 'tasks/visual-codeprose/subject0000/session000/subject0_session0_behOutput_2021-04-02-14.28.30.csv',\n",
    "    data_dir / 'tasks/visual-codeprose/subject0001/session000/subject1_session0_behOutput_2021-03-26-14.31.14.csv',\n",
    "]\n",
    "\n",
    "def _build_breaks(df):\n",
    "    starts = df['t_answered'].iloc[:-1].shift()\n",
    "    starts_utc = df['t_answered_utc'].iloc[:-1].shift()\n",
    "    stops = df['t_presented'].iloc[1:]\n",
    "    stops_utc = df['t_presented_utc'].iloc[1:]\n",
    "    \n",
    "    breaks = pd.DataFrame({\n",
    "        \"t_presented\": starts, \n",
    "        \"t_answered\": stops, \n",
    "        \"t_presented_utc\": starts_utc, \n",
    "        \"t_answered_utc\": stops_utc, \n",
    "        \"type\": \"relax\", \n",
    "        \"duration\": stops - starts, \n",
    "        'subject': df['subject'],\n",
    "        'image_path': 'none',\n",
    "        'response': 'up',  # as placeholder\n",
    "    })\n",
    "    return breaks\n",
    "\n",
    "dfs = []\n",
    "for file in marker_files:\n",
    "    df = pd.read_csv(file, index_col=0)\n",
    "    df['duration'] = df['t_answered'] - df['t_presented']\n",
    "    match = re.search('subject(\\d+)', str(file))\n",
    "    assert match\n",
    "    df['subject'] = int(match.group(1))\n",
    "    \n",
    "    if classify_breaks:\n",
    "        breaks = _build_breaks(df)\n",
    "        df = df.append(breaks)\n",
    "    dfs.append(df)\n",
    "df_markers = pd.concat(dfs).sort_values(by=['subject', 't_presented'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at some of the marker rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_markers['img'] = df_markers['image_path'].apply((lambda c: c.split(\"/\")[-1]))\n",
    "df_markers.drop(columns=['image_path']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Now we need to preprocess the data a bit, gathering the EEG data for each trial in the experiment.\n",
    "\n",
    " - [ ] Better cleaning/rejection of bad epochs/windows/samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter short trials\n",
    "\n",
    "We filter away rows where the subject didn't spend at least `min_task_duration` seconds with the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prev = len(df_markers)\n",
    "df_markers = df_markers[df_markers['duration'] > min_task_duration]\n",
    "print(f\"Filtered away {n_prev - len(df_markers)} epochs due to short duration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter no answer trials\n",
    "\n",
    "We filter away rows where space was clicked (didn't answer/skipped/unsure?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prev = len(df_markers)\n",
    "df_markers = df_markers[df_markers['response'].isin(['up', 'down'])]\n",
    "print(f\"Filtered away {n_prev - len(df_markers)} epochs due skipped by subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if single_subject is not None:\n",
    "    logger.info(f\"Selecting subject {single_subject}\")\n",
    "    n_prev = len(df_markers)\n",
    "    new_markers = df_markers[df_markers['subject'] == single_subject]\n",
    "    if not new_markers.empty:\n",
    "        df_markers = new_markers\n",
    "    print(f\"old size: {n_prev}, new size: {len(df_markers)}\")\n",
    "else:\n",
    "    logger.info(\"Using all subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential moving standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if standardize:\n",
    "    from braindecode.datautil import exponential_moving_standardize\n",
    "    print(eeg.to_numpy())\n",
    "    output = exponential_moving_standardize(eeg.to_numpy())\n",
    "    print(output)\n",
    "    \n",
    "    for ch_idx, col in enumerate(eeg.columns):\n",
    "        eeg[col] = output[:, ch_idx]\n",
    "else:\n",
    "    logger.info(\"Skipping exponential standardize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bandpass filtering\n",
    "\n",
    " - [ ] FIXME: Why is the signal shifted after filtering? Should this be accounted for somehow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandpass-filter the signal\n",
    "if use_bandpass_filter:\n",
    "    logger.info(\"Bandpass-filtering the signal\")\n",
    "    plot_eeg(eeg.to_numpy(), 20000, 10 * sfreq)\n",
    "    \n",
    "    eeg_clean = clean.filter(eeg)\n",
    "    for ch_idx, col in enumerate(eeg.columns):\n",
    "        eeg[col] = eeg_clean[:, ch_idx]\n",
    "        \n",
    "    # plot the new result\n",
    "    plot_eeg(eeg.to_numpy(), 20000, 10 * sfreq)\n",
    "else:\n",
    "    logger.info(\"Skipping bandpass filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing epochs\n",
    "\n",
    "Now we match up the EEG data with the markers to create our epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "for _, row in df_markers.iterrows():\n",
    "    start = datetime.fromtimestamp(row['t_presented_utc'], timezone.utc)\n",
    "    stop = datetime.fromtimestamp(row['t_answered_utc'], timezone.utc)\n",
    "    epoch = eeg.truncate(start, stop)\n",
    "    \n",
    "    # Check that sample count aligns with epoch duration\n",
    "    expected_samples = round(row['duration'] * sfreq)\n",
    "    actual_samples = len(epoch)\n",
    "    diff = expected_samples - actual_samples\n",
    "    if abs(diff) > 5:\n",
    "        logger.warning(f\"Expected {expected_samples} samples, found {actual_samples}\")\n",
    "        \n",
    "    epochs.append((epoch, row['type'], row['subject'], row['img']))\n",
    "print(f\"epochs: {len(epochs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epochs to windows\n",
    "\n",
    "Now we split up the epochs into windows of a fixed size.\n",
    "\n",
    " - [ ] TODO: Use a sliding window approach, similar to the one in braindecode (needs care with CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split epochs into windows\n",
    "WINDOW_SIZE = 512\n",
    "\n",
    "windows = []\n",
    "for epoch, type, subject, img in epochs:\n",
    "    for i in range(0, len(epoch), WINDOW_SIZE):\n",
    "        window = epoch.iloc[i:i+WINDOW_SIZE]\n",
    "        if len(window) == WINDOW_SIZE:\n",
    "            windows.append((window, type, subject, img))\n",
    "        else:\n",
    "            logger.debug(f'epoch too small ({len(window)}), skipping')\n",
    "print(f\"windows: {len(windows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing our `X` and `y`\n",
    " \n",
    "Now to actually construct matrices that we can feed into the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, subj, img = zip(*windows)\n",
    "X = np.array([x.to_numpy().T for x in X])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a look at the signal in a few of the windows to make sure it looks ok\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "axs = fig.subplots(4, 1, sharex=True, sharey=True)\n",
    "for i, ax in enumerate(axs):\n",
    "    plt.ylim(-40, 40)\n",
    "    plt.xlim(0, WINDOW_SIZE)\n",
    "    ax.plot(X[i, :, :].T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "print(y.shape)\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = np.array(subj)\n",
    "img = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import shuffle\n",
    "\n",
    "if balance_dataset:\n",
    "    assert len(set(y)) == 2, 'only supports two classes'\n",
    "    \n",
    "    c_y: dict = Counter(y)\n",
    "    c_smallest, min_count = min(c_y.items(), key=lambda t: t[1])\n",
    "    print(f\"Smallest class: {c_smallest} with {min_count} samples\")\n",
    "    \n",
    "    large_class_samples = np.argwhere(y != c_smallest).flatten()\n",
    "    other_class_samples = np.argwhere(y == c_smallest).flatten()\n",
    "    \n",
    "    # Randomly undersample the largest class\n",
    "    shuffle(large_class_samples)\n",
    "    subsample = large_class_samples[:min_count]\n",
    "    idx = np.concatenate([subsample, other_class_samples])\n",
    "    X, y, subj, img = X[idx], y[idx], subj[idx], img[idx]\n",
    "    print(f\"Total samples: {len(y)}\")\n",
    "    \n",
    "    # Doesn't work as fit_resample expects X to be a 2D matrix\n",
    "    #from imblearn.under_sampling import RandomUnderSampler\n",
    "    #rus = RandomUnderSampler(random_state=0)\n",
    "    #X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "    #print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Here we train our model using pyRiemann.\n",
    "\n",
    " - [ ] Much of this code is from eegclassify/main.py, code should probably be reused better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set up the different classifiers we want to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import LeaveOneGroupOut, learning_curve\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "from pyriemann.estimation import Covariances, ERPCovariances, XdawnCovariances\n",
    "from pyriemann.spatialfilters import CSP\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "\n",
    "\n",
    "# Fixes non-convergence for binary classification\n",
    "dual = set(y) == 2\n",
    "\n",
    "clfs: Dict[str, Pipeline] = {\n",
    "    # These four are from https://neurotechx.github.io/eeg-notebooks/auto_examples/visual_ssvep/02r__ssvep_decoding.html\n",
    "    \"CSP + Cov + TS\": make_pipeline(\n",
    "        Covariances(),\n",
    "        CSP(4, log=False),\n",
    "        TangentSpace(),\n",
    "        LogisticRegression(dual=dual),\n",
    "    ),\n",
    "    \"Cov + TS\": make_pipeline(\n",
    "        Covariances(), TangentSpace(), LogisticRegression(dual=dual)\n",
    "    ),\n",
    "    \"Xdawn + TS\": make_pipeline(\n",
    "        XdawnCovariances(2),\n",
    "        TangentSpace(metric='riemann'),\n",
    "        LogisticRegression()\n",
    "    ),\n",
    "    # Performs meh\n",
    "    #\"CSP + RegLDA\": make_pipeline(\n",
    "    #    Covariances(), CSP(4), LDA(shrinkage=\"auto\", solver=\"eigen\")\n",
    "    #),\n",
    "    # Performs badly\n",
    "    # \"Cov + MDM\": make_pipeline(Covariances(), MDM()),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we train each classifier and plot their respective confusion matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from eegclassify.util import unison_shuffled_copies\n",
    "from eegclassify.main import _performance\n",
    "\n",
    "for name, clf in clfs.items():\n",
    "    logger.info(f\"===== Training with {name} =====\")\n",
    "\n",
    "    # Shuffled split\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "        X, y, test_size=0.3, shuffle=True\n",
    "    )\n",
    "\n",
    "    logger.info(\"Training...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    logger.info(f\"Test score: {clf.score(X_test, y_test)}\")\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    perf = _performance(y_test, y_pred)\n",
    "    logger.info(perf)\n",
    "    \n",
    "    disp = plot_confusion_matrix(clf, X_test, y_test,\n",
    "                                 display_labels=['code', 'prose'],\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize='true')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LORO/LOGO split\n",
    "# TODO: Is LOGO the same as LORO?\n",
    "logo = LeaveOneGroupOut()\n",
    "groups = img\n",
    "\n",
    "for name, clf in clfs.items():\n",
    "    logger.info(f\"===== Training with {name} =====\")\n",
    "    \n",
    "    score = sklearn.model_selection.cross_val_score(clf, X, y, cv=3, n_jobs=-1)\n",
    "    logger.info(f\"CV score (shuffled):   {score}\")\n",
    "    \n",
    "    # LORO\n",
    "    score = sklearn.model_selection.cross_val_score(clf, X, y, cv=logo, groups=groups, n_jobs=-1)\n",
    "    logger.info(f\"CV score (LORO):       {np.mean(score) :.3f} (mean), {np.std(score) :.3f} (std)\")\n",
    "    \n",
    "    # FIXME: This shouldn't run on the entire sample\n",
    "    plot_confusion_matrix(clf, X, y,\n",
    "                          display_labels=['code', 'prose'],\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          normalize='true')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves\n",
    "\n",
    "Now to check the learning curves and see if the train and validation scores converge.\n",
    "\n",
    "**Note:** Performance is currently terrible as there isn't enough data for the model to learn to generalize across subjects (easily seen by changing to shuffled CV).\n",
    "\n",
    "A great example of how to plot learning curves is available here: https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eegclassify.util import powspace\n",
    "\n",
    "for name, clf in clfs.items():\n",
    "    logger.info(f\"===== Training with {name} =====\")\n",
    "    \n",
    "    # We create shuffled versions of the dataset to ensure that all stimuli of the same type aren't in sequence\n",
    "    # (as is the case for subject 1 which didn't have shuffled stimuli)\n",
    "    x_l, y_l, groups_l = unison_shuffled_copies(X, y, groups)\n",
    "    #x_l, y_l, groups_l = X, y, groups\n",
    "    \n",
    "    # We build a train_sizes to train across several sample sizes\n",
    "    groups_c: dict = Counter(groups_l)\n",
    "    largest_group, largest_group_count = max(groups_c.items(), key=lambda g: g[1])\n",
    "    largest_loro_train = len(y) - largest_group_count\n",
    "    smallest_train = 10\n",
    "    \n",
    "    train_sizes = np.floor(powspace(smallest_train, largest_loro_train, power=4, num=10)) / largest_loro_train\n",
    "    # print(train_sizes)\n",
    "    \n",
    "    # Compute the learning curve\n",
    "    train_sizes, train_scores, valid_scores = learning_curve(\n",
    "        clf, x_l, y_l, groups=groups_l, \n",
    "        train_sizes=train_sizes, \n",
    "        cv=logo, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    m = np.mean(train_scores, axis=1)\n",
    "    std = np.std(train_scores, axis=1)\n",
    "    plt.plot(train_sizes, m, label=\"training score\", marker='.')\n",
    "    plt.fill_between(train_sizes, m-std, m+std, alpha=0.2)\n",
    "    \n",
    "    m = np.mean(valid_scores, axis=1)\n",
    "    std = np.std(valid_scores, axis=1)\n",
    "    plt.plot(train_sizes, m, label=\"validation score\", marker='.')\n",
    "    plt.fill_between(train_sizes, m-std, m+std, alpha=0.2)\n",
    "    \n",
    "    plt.axhline(0.5, color='grey', linestyle='--', linewidth=0.8)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlim(train_sizes[0], train_sizes[-1])\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Braindecode stuff\n",
    "\n",
    "Here we'll experiment with braindecode (convnets) to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use braindecode we need to transform our data to the braindecode format\n",
    "\n",
    "from braindecode.datautil import create_from_X_y\n",
    "\n",
    "# This wants X to be in the shape (x_trials, n_channels, n_samples)\n",
    "Xb, yb, subjb, imgb = zip(*epochs)\n",
    "Xb = [x.to_numpy().T for x in Xb]\n",
    "print(len(Xb), Xb[0].shape)\n",
    "yb = np.array([0 if yy == 'code' else 1 for yy in y])\n",
    "print(yb.shape)\n",
    "windows_dataset = create_from_X_y(\n",
    "    Xb, yb, drop_last_window=False, sfreq=sfreq, ch_names=list(eeg.columns),\n",
    "    window_stride_samples=500,\n",
    "    window_size_samples=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_dataset.description['group'] = imgb\n",
    "windows_dataset.description.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homegrown LORO\n",
    "splitted = windows_dataset.split('group')\n",
    "train_sets = [splitted[key] for key in list(splitted.keys())[1:]]\n",
    "train_set = train_sets[0]\n",
    "for ts in train_sets[1:]:\n",
    "    train_sets += ts\n",
    "valid_set = splitted[list(splitted.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from braindecode.util import set_random_seeds\n",
    "from braindecode.models import ShallowFBCSPNet\n",
    "\n",
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "if cuda:\n",
    "    print(\"CUDA available!\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "# Set random seed to be able to reproduce results\n",
    "seed = 20200220\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "# Extract number of chans and time steps from dataset\n",
    "n_classes = len(set(y))\n",
    "n_chans = train_set[0][0].shape[0]\n",
    "input_window_samples = train_set[0][0].shape[1]\n",
    "\n",
    "print(f\"classes:   {n_classes}\")\n",
    "print(f\"channels:  {n_chans}\")\n",
    "print(f\"samples per window:  {input_window_samples}\")\n",
    "\n",
    "model = ShallowFBCSPNet(\n",
    "    n_chans,\n",
    "    n_classes,\n",
    "    input_window_samples=input_window_samples,\n",
    "    final_conv_length='auto',\n",
    ")\n",
    "\n",
    "# Send model to GPU\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set[0])\n",
    "print(train_set[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "\n",
    "from braindecode import EEGClassifier\n",
    "\n",
    "# These values we found good for shallow network:\n",
    "lr = 0.0625 * 0.01\n",
    "weight_decay = 0\n",
    "\n",
    "# For deep4 they should be:\n",
    "# lr = 1 * 0.01\n",
    "# weight_decay = 0.5 * 0.001\n",
    "\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "\n",
    "clf = EEGClassifier(\n",
    "    model,\n",
    "    criterion=torch.nn.NLLLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    train_split=predefined_split(valid_set),  # using valid_set for validation\n",
    "    optimizer__lr=lr,\n",
    "    optimizer__weight_decay=weight_decay,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "    ],\n",
    "    device='cuda' if cuda else 'cpu',\n",
    ")\n",
    "\n",
    "# Model training for a specified number of epochs. `y` is None as it is already supplied in the dataset.\n",
    "# FIXME: Remove try/except when error is resolved\n",
    "try:\n",
    "    clf.fit(train_set, y=None, epochs=n_epochs)\n",
    "except Exception as e:\n",
    "    logger.exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erb-thesis",
   "language": "python",
   "name": "erb-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
