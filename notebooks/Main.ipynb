{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main analysis\n",
    "\n",
    "The primary analysis for the thesis, where we train a classifier for the code vs prose task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from eegclassify import main, load, clean, features, preprocess\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set this to True to run on testing data\n",
    "simulate_test = False\n",
    "if simulate_test:\n",
    "    import os\n",
    "    os.environ['PYTEST_CURRENT_TEST'] = \"true\"\n",
    "    \n",
    "# Configuration\n",
    "use_bandpass_filter = True\n",
    "classify_breaks = False\n",
    "    \n",
    "# Constants\n",
    "sfreq = 256  # sampling frequency of the Muse S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "document.title='erb-thesis/Main - Jupyter'  // Set the document title to be able to track time spent working on the notebook with ActivityWatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading EEG data\n",
    "\n",
    "First we need to load the EEG data used during the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data').resolve()\n",
    "\n",
    "# Available in git\n",
    "file_example = data_dir / 'eeg/muse/subject0000/session001/recording_2021-04-02-14.03.36.csv'\n",
    "\n",
    "# Private data\n",
    "files_private = []\n",
    "path_private = Path('/home/erb/.eegnb/data/test/local/museS/subject0001/session001/')\n",
    "if path_private.exists():\n",
    "    files_private = list(path_private.glob('*.csv'))\n",
    "    \n",
    "files = [\n",
    "    file_example,\n",
    "    *files_private,\n",
    "]\n",
    "\n",
    "eeg = load.load_eeg(files)\n",
    "eeg = eeg.set_index('timestamp').sort_index()\n",
    "print(eeg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading markers\n",
    "\n",
    "Now we need to load the markers produced during the experiment, so we can annotate the EEG data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_files = [\n",
    "    data_dir / 'tasks/visual-codeprose/subject0000/session000/subject0_session0_behOutput_2021-04-02-14.28.30.csv',\n",
    "]\n",
    "\n",
    "marker_files_private = [\n",
    "    Path('~/.eegnb/data/visual-codeprose/local/none/session000/subject1_session0_behOutput_2021-03-26-14.31.14.csv').expanduser()\n",
    "]\n",
    "for file in marker_files_private:\n",
    "    if file.exists():\n",
    "        marker_files.append(file)\n",
    "\n",
    "\n",
    "def _build_breaks(df):\n",
    "    starts = df['t_answered'].iloc[:-1].shift()\n",
    "    starts_utc = df['t_answered_utc'].iloc[:-1].shift()\n",
    "    stops = df['t_presented'].iloc[1:]\n",
    "    stops_utc = df['t_presented_utc'].iloc[1:]\n",
    "    \n",
    "    breaks = pd.DataFrame({\n",
    "        \"t_presented\": starts, \n",
    "        \"t_answered\": stops, \n",
    "        \"t_presented_utc\": starts_utc, \n",
    "        \"t_answered_utc\": stops_utc, \n",
    "        \"type\": \"relax\", \n",
    "        \"duration\": stops - starts, \n",
    "        'subject': df['subject'],\n",
    "        'image_path': 'none',\n",
    "        'response': 'up',  # as placeholder\n",
    "    })\n",
    "    return breaks\n",
    "\n",
    "dfs = []\n",
    "for file in marker_files:\n",
    "    df = pd.read_csv(file, index_col=0)\n",
    "    df['duration'] = df['t_answered'] - df['t_presented']\n",
    "    match = re.search('subject(\\d+)', str(file))\n",
    "    assert match\n",
    "    df['subject'] = int(match.group(1))\n",
    "    \n",
    "    if classify_breaks:\n",
    "        breaks = _build_breaks(df)\n",
    "        df = df.append(breaks)\n",
    "    dfs.append(df)\n",
    "df_markers = pd.concat(dfs).sort_values(by=['subject', 't_presented'])\n",
    "\n",
    "# Filter away rows where the subject didn't spend at least 10 seconds with the task\n",
    "n_prev = len(df_markers)\n",
    "df_markers = df_markers[df_markers['duration'] > 5]\n",
    "print(f\"Filtered away {n_prev - len(df_markers)} epochs due to short duration\")\n",
    "\n",
    "# Filter away rows where space was clicked (didn't answer/skipped/unsure?)\n",
    "n_prev = len(df_markers)\n",
    "df_markers = df_markers[df_markers['response'].isin(['up', 'down'])]\n",
    "print(f\"Filtered away {n_prev - len(df_markers)} epochs due skipped by subject\")\n",
    "\n",
    "df_markers['img'] = df_markers['image_path'].apply((lambda c: c.split(\"/\")[-1]))\n",
    "\n",
    "# Preview first 5 rows\n",
    "df_markers.drop(columns=['image_path']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Now we need to preprocess the data a bit, gathering the EEG data for each epoch in the experiment.\n",
    "\n",
    " - [ ] Better cleaning/rejection of bad epochs/windows/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "plt.matshow(eeg.to_numpy()[:sfreq, :].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandpass-filter the signal\n",
    "if use_bandpass_filter:\n",
    "    eeg_clean = clean.filter(eeg)\n",
    "    for ch_idx, col in enumerate(eeg.columns):\n",
    "        eeg[col] = eeg_clean[:, ch_idx]\n",
    "        \n",
    "    # plot the new result\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.matshow(eeg.to_numpy()[:sfreq, :].T)\n",
    "else:\n",
    "    print(\"Bandpass filtering was skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "for _, row in df_markers.iterrows():\n",
    "    start = datetime.fromtimestamp(row['t_presented_utc'], timezone.utc)\n",
    "    stop = datetime.fromtimestamp(row['t_answered_utc'], timezone.utc)\n",
    "    epoch = eeg.truncate(start, stop)\n",
    "    \n",
    "    # Check that sample count aligns with epoch duration\n",
    "    expected_samples = round(row['duration'] * sfreq)\n",
    "    actual_samples = len(epoch)\n",
    "    diff = expected_samples - actual_samples\n",
    "    if abs(diff) > 5:\n",
    "        logger.warning(f\"Expected {expected_samples} samples, found {actual_samples}\")\n",
    "        \n",
    "    epochs.append((epoch, row['type'], row['subject']))\n",
    "print(len(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split epochs into windows\n",
    "WINDOW_SIZE = 512\n",
    "\n",
    "windows = []\n",
    "for epoch, type, subject in epochs:\n",
    "    for i in range(0, len(epoch), WINDOW_SIZE):\n",
    "        window = epoch.iloc[i:i+WINDOW_SIZE]\n",
    "        if len(window) == WINDOW_SIZE:\n",
    "            windows.append((window, type, subject))\n",
    "        else:\n",
    "            print(f'epoch too small ({len(window)}), skipping')\n",
    "print(len(windows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing our dataset\n",
    " \n",
    " - [ ] Split into sessions (by date) for LORO CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our X and y\n",
    "\n",
    "X, y, subj = zip(*windows)\n",
    "X = np.array([x.to_numpy().T for x in X])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "y = np.array(y)\n",
    "#y = np.array([0 if yy == 'prose' else 1 for yy in y])\n",
    "print(y.shape)\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = np.array(subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.matshow(X[0, :, :sfreq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model\n",
    "\n",
    "Here we train our model using pyRiemann.\n",
    "\n",
    " - [ ] Much of this code is from eegclassify/main.py, code should probably be reused better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import LeaveOneGroupOut, learning_curve\n",
    "\n",
    "from pyriemann.estimation import Covariances, ERPCovariances, XdawnCovariances\n",
    "from pyriemann.spatialfilters import CSP\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "\n",
    "\n",
    "# Fixes non-convergence for binary classification\n",
    "dual = set(y) == 2\n",
    "\n",
    "clfs: Dict[str, Pipeline] = {\n",
    "    # These four are from https://neurotechx.github.io/eeg-notebooks/auto_examples/visual_ssvep/02r__ssvep_decoding.html\n",
    "    \"CSP + Cov + TS\": make_pipeline(\n",
    "        Covariances(),\n",
    "        CSP(4, log=False),\n",
    "        TangentSpace(),\n",
    "        LogisticRegression(dual=dual),\n",
    "    ),\n",
    "    \"Cov + TS\": make_pipeline(\n",
    "        Covariances(), TangentSpace(), LogisticRegression(dual=dual)\n",
    "    ),\n",
    "    # Performs meh\n",
    "    # \"CSP + RegLDA\": make_pipeline(\n",
    "    #     Covariances(), CSP(4), LDA(shrinkage=\"auto\", solver=\"eigen\")\n",
    "    # ),\n",
    "    # Performs badly\n",
    "    # \"Cov + MDM\": make_pipeline(Covariances(), MDM()),\n",
    "}\n",
    "    \n",
    "def unison_shuffled_copies(a, b, c):\n",
    "    assert len(a) == len(b) == len(c)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p], c[p]\n",
    "    \n",
    "cv_method = \"LOGO\"\n",
    "\n",
    "for name, clf in clfs.items():\n",
    "    logger.info(f\"===== Training with {name} =====\")\n",
    "    \n",
    "    # LORO/LOGO split\n",
    "    # TODO: Is LOGO the same as LORO?\n",
    "    logo = LeaveOneGroupOut()\n",
    "    # x_idx, y_idx = logo.split(X, y, subj)\n",
    "\n",
    "    # Shuffled split\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "        X, y, test_size=0.3, shuffle=True\n",
    "    )\n",
    "\n",
    "    logger.info(\"Training...\")\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    logger.info(f\"Test score: {clf.score(X_test, y_test)}\")\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    from eegclassify.main import _performance\n",
    "    perf = _performance(y_test, y_pred)\n",
    "    logger.info(perf)\n",
    "\n",
    "    score = sklearn.model_selection.cross_val_score(clf, X, y, cv=3)\n",
    "    logger.info(f\"CV score (shuffled): {score}\")\n",
    "    \n",
    "    # LORO\n",
    "    score = sklearn.model_selection.cross_val_score(clf, X, y, cv=LeaveOneGroupOut(), groups=subj)\n",
    "    logger.info(f\"CV score (LORO):     {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves\n",
    "\n",
    "Now to check the learning curves and see if the train and validation scores converge.\n",
    "\n",
    "**Note:** Performance is currently terrible as there isn't enough data for the model to learn to generalize across subjects (easily seen by changing to shuffled CV).\n",
    "\n",
    "A great example of how to plot learning curves is available here: https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in clfs.items():\n",
    "    logger.info(f\"===== Training with {name} =====\")\n",
    "    \n",
    "    # We create shuffled versions of the dataset to ensure that all stimuli of the same type aren't in sequence\n",
    "    # (as is the case for subject 1 which didn't have shuffled stimuli)\n",
    "    x_l, y_l, subj_l = unison_shuffled_copies(X, y, subj)\n",
    "    \n",
    "    # Compute the learning curve\n",
    "    train_sizes, train_scores, valid_scores = learning_curve(\n",
    "        clf, x_l, y_l, groups=subj_l, \n",
    "        train_sizes=range(50, 500, 50), cv=logo\n",
    "    )\n",
    "    \n",
    "    plt.plot(train_sizes, train_scores, label=\"train score\")\n",
    "    plt.plot(train_sizes, valid_scores, label=\"valid score\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Braindecode stuff\n",
    "\n",
    "Here we'll experiment with braindecode (convnets) to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil import create_from_X_y\n",
    "\n",
    "# This wants X to be in the shape (x_trials, n_channels, n_samples)\n",
    "X, y, subj = zip(*epochs)\n",
    "X = [x.to_numpy().T for x in X]\n",
    "print(len(X), X[0].shape)\n",
    "y = np.array(y)\n",
    "print(y.shape)\n",
    "windows_dataset = create_from_X_y(\n",
    "    X, y, drop_last_window=False, sfreq=sfreq, ch_names=list(eeg.columns),\n",
    "    window_stride_samples=500,\n",
    "    window_size_samples=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_dataset.description['subject'] = subj\n",
    "windows_dataset.description.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted = windows_dataset.split('subject')\n",
    "train_set = splitted['0']\n",
    "valid_set = splitted['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from braindecode.util import set_random_seeds\n",
    "from braindecode.models import ShallowFBCSPNet\n",
    "\n",
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "if cuda:\n",
    "    print(\"CUDA available!\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "# Set random seed to be able to reproduce results\n",
    "seed = 20200220\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "# Extract number of chans and time steps from dataset\n",
    "n_classes = len(set(y))\n",
    "n_chans = train_set[0][0].shape[0]\n",
    "input_window_samples = train_set[0][0].shape[1]\n",
    "\n",
    "print(f\"classes:   {n_classes}\")\n",
    "print(f\"channels:  {n_chans}\")\n",
    "print(f\"samples per window:  {input_window_samples}\")\n",
    "\n",
    "model = ShallowFBCSPNet(\n",
    "    n_chans,\n",
    "    n_classes,\n",
    "    input_window_samples=input_window_samples,\n",
    "    final_conv_length='auto',\n",
    ")\n",
    "\n",
    "# Send model to GPU\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set[0])\n",
    "print(train_set[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "\n",
    "from braindecode import EEGClassifier\n",
    "\n",
    "# These values we found good for shallow network:\n",
    "lr = 0.0625 * 0.01\n",
    "weight_decay = 0\n",
    "\n",
    "# For deep4 they should be:\n",
    "# lr = 1 * 0.01\n",
    "# weight_decay = 0.5 * 0.001\n",
    "\n",
    "batch_size = 64\n",
    "n_epochs = 4\n",
    "\n",
    "clf = EEGClassifier(\n",
    "    model,\n",
    "    criterion=torch.nn.NLLLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    train_split=predefined_split(valid_set),  # using valid_set for validation\n",
    "    optimizer__lr=lr,\n",
    "    optimizer__weight_decay=weight_decay,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "    ],\n",
    "    device='cuda' if cuda else 'cpu',\n",
    ")\n",
    "\n",
    "# Model training for a specified number of epochs. `y` is None as it is already supplied in the dataset.\n",
    "# FIXME: Remove try/except when error is resolved\n",
    "try:\n",
    "    clf.fit(train_set, y=None, epochs=n_epochs)\n",
    "except Exception as e:\n",
    "    logger.exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erb-thesis",
   "language": "python",
   "name": "erb-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
